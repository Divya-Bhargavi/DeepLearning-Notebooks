{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to these blog-posts for an indepth understanding of RNNS and LSTM's    \n",
    "https://github.com/karpathy/char-rnn <br>\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 <br>\n",
    "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:58:58.728409Z",
     "start_time": "2019-05-06T01:58:52.782101Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:58:58.735252Z",
     "start_time": "2019-05-06T01:58:58.730915Z"
    }
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('HP1.txt', 'r' ,encoding=\"utf8\", errors='ignore') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:58:58.747940Z",
     "start_time": "2019-05-06T01:58:58.736997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T07:05:23.985962Z",
     "start_time": "2018-12-27T07:05:23.979784Z"
    }
   },
   "source": [
    "### Encoding the text\n",
    "\n",
    "We will convert all the characters in the text to numbers to be fed into the network\n",
    "> * First we index the set of characters we have in set\n",
    "* Then we encode the text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:00.710185Z",
     "start_time": "2019-05-06T01:59:00.653360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique characters from text\n",
    "chars = tuple(set(text))\n",
    "# Get int to chars mapping\n",
    "int2char = dict(enumerate(chars))\n",
    "# Reverse the above to get numeric values for characters\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:01.235383Z",
     "start_time": "2019-05-06T01:59:01.231442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T07:06:02.400444Z",
     "start_time": "2018-12-29T07:06:02.397768Z"
    }
   },
   "source": [
    "Check the double 'r' in Harry repeating as two 8's in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:02.107690Z",
     "start_time": "2019-05-06T01:59:02.102211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 39, 66, 66, 13, 24, 64, 53, 43, 43, 50, 66, 24, 39, 27, 73, 24,\n",
       "       43, 51, 50, 24,  2, 53, 66,  7, 50, 66, 50, 66, 60, 28, 24,  2, 43,\n",
       "       53, 27, 50, 24, 55, 55, 18, 69, 25, 64, 45, 52, 30, 24, 71, 32, 52,\n",
       "       24, 55, 55, 45, 69, 52, 24, 44, 71, 65, 24,  5, 69, 71, 24, 61,  9,\n",
       "       20, 52, 36, 24, 55, 55, 31, 66, 56, 24, 39, 27, 73, 24, 31, 66, 28,\n",
       "       56, 24, 36, 74, 66, 28, 38, 50, 13, 11, 24, 53,  4, 24, 27])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T07:11:47.231965Z",
     "start_time": "2018-12-29T07:11:47.227357Z"
    }
   },
   "source": [
    ">* To making this a multi-class classification with 78 classes (one of each unique character),\n",
    "we convert every number encoded into an array of size 78 with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:03.035639Z",
     "start_time": "2019-05-06T01:59:03.031257Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(input_seq, num_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot_vector = np.zeros((np.multiply(*input_seq.shape), num_labels), dtype=np.float32)\n",
    "    # Fill rows with 1 only on the index that matches the number\n",
    "    one_hot_vector[np.arange(one_hot_vector.shape[0]), input_seq.flatten()] = 1.  \n",
    "    # Reshape original\n",
    "    one_hot_vector = one_hot_vector.reshape((*input_seq.shape, num_labels))\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:03.372795Z",
     "start_time": "2019-05-06T01:59:03.368994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing function\n",
    "input_seq = np.array([[1,2,3],[2,3,6],[2,5,6],[6,3,8],[3,4,6]])\n",
    "num_labels = 10\n",
    "temp = one_hot_encoding(input_seq, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:03.668198Z",
     "start_time": "2019-05-06T01:59:03.665408Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:13.663352Z",
     "start_time": "2019-05-06T01:59:13.658561Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-18fbfe91648d>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-18fbfe91648d>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    y =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(input_seq.flatten())//(batch_size*seq_length)\n",
    "input_seq = input_seq.flatten()[:num_batches*batch_size*seq_length]\n",
    "input_seq = input_seq.reshape(batch_size,-1)\n",
    "\n",
    "for i in range(0,input_seq.shape[1], seq_length):\n",
    "    x = arr[:, i:i+seq_length]\n",
    "    if i+ seq_length == input_seq.shape[1]:\n",
    "    y = \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:50.153640Z",
     "start_time": "2019-05-06T01:59:50.145484Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 15 into shape (2,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2e07a5fd4696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 15 into shape (2,newaxis)"
     ]
    }
   ],
   "source": [
    "input_seq.reshape(batch_size,-1)[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DataLoader that yields batches everytime it is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check the Sentiment analysis notebook to know how to wrap data into a torch Dataset to be used by a torch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:54.691497Z",
     "start_time": "2019-05-06T01:59:54.685293Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(input_seq, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    This function generates (batch_size * seq_length) batches from\n",
    "    input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    num_batches = len(input_seq.flatten())//(batch_size*seq_length)\n",
    "    # Remove the extra characters from input_seq that donot fit batch size\n",
    "    input_seq = input_seq.flatten()[:num_batches*batch_size*seq_length]\n",
    "    input_seq = input_seq.reshape(batch_size,-1)\n",
    "    \n",
    "    for i in range(0,input_seq.shape[1], seq_length):\n",
    "        x = input_seq[:, i:i+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], input_seq[:, i+seq_length]\n",
    "        # feed the first column , when there is no more character to be assigned to y in the end \n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], input_seq[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:55.408880Z",
     "start_time": "2019-05-06T01:59:55.388527Z"
    }
   },
   "outputs": [],
   "source": [
    "batches = data_loader(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:55.954806Z",
     "start_time": "2019-05-06T01:59:55.950632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 50), (8, 50))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T01:59:58.888147Z",
     "start_time": "2019-05-06T01:59:57.386311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('Training on CPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Class for character LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T09:04:17.930607Z",
     "start_time": "2018-12-29T09:04:17.926952Z"
    }
   },
   "source": [
    "https://pytorch.org/docs/stable/nn.html#lstm\n",
    "> * n_layer --> number of layers of LSTM, for n_layers > 1, LSTMS are stacked\n",
    "* n_hidden --> number of neurons in hidden layer\n",
    "* dropout_prob --> Probabilty for drop out in LSTM\n",
    "* batch_first --> the input and output tensors are provided as (batch, seq, feature)\n",
    "\n",
    "\n",
    "> Dimensions:\n",
    "* Input --> (batch_size, seq_length, input_size)\n",
    "* hidden --> (n_layers, batch_size, hidden_dim)\n",
    "* Output --> (batch_size, seq_length, hidden_size)\n",
    "\n",
    "* After LSTM we have a fully connect layer that reduces the output dimension from hidden layer dim to input sequence unit dim \n",
    "* We return the hidden unit after forward pass to be fed to next LSTM\n",
    "* We initilize initial weights for cell state and hidden state with zero's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:02:04.558829Z",
     "start_time": "2019-05-06T02:02:04.547096Z"
    }
   },
   "outputs": [],
   "source": [
    "class charLSTM(nn.Module):\n",
    "    def __init__(self, chars, n_hidden=256, n_layers=2,\n",
    "                               dropout_prob=0.5, lr=0.001):\n",
    "        super(charLSTM, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Encode\n",
    "        self.chars = chars\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # check the LSTM parameters from pytorch\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to get out put in the dimension of encoding\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass input and hidden state.\n",
    "        Define hidden state intially.\n",
    "        \"\"\"      \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden\n",
    "        # with same type as weights initialized to zero, for hidden state and cell state of LSTM\n",
    "        # check https://discuss.pytorch.org/t/when-to-initialize-lstm-hidden-state/2323/8 for clarification\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:16:57.873387Z",
     "start_time": "2019-05-06T02:16:57.857046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charLSTM(\n",
      "  (lstm): LSTM(78, 200, num_layers=4, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=200, out_features=78, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden= 200\n",
    "n_layers=4\n",
    "\n",
    "net = charLSTM(chars, n_hidden, n_layers)\n",
    "if(train_on_gpu):\n",
    "        net = net.cuda()\n",
    "        \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Before training, we split the data into training and validation set\n",
    "* Our perfomance metric here will just be the loss because, we take the absolute probabilities of varies characters in an output vector and sample from it for next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:16:59.344747Z",
     "start_time": "2019-05-06T02:16:59.341475Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_percent = 0.2\n",
    "valid_idx = int(len(encoded)*(1 - valid_percent))\n",
    "train_encoded = encoded[:valid_idx]\n",
    "valid_encoded = encoded[valid_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:16:59.641167Z",
     "start_time": "2019-05-06T02:16:59.631948Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_on_gpu,net,train_data, valid_data, epochs = 1, seq_length = 30, \n",
    "          batch_size = 256, lr = 0.001, clip = 5, name = str('CharLSTM')):\n",
    "    \"\"\"\n",
    "    Function to train the LSTM in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = 10^8\n",
    "    char_len = len(net.chars)\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        # Set model to train mode\n",
    "        net.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        \n",
    "        # Initial hidden state filled with zeros\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for i, (x, y) in enumerate(data_loader(train_data, batch_size, seq_length)):\n",
    "            \n",
    "            optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "            batch = y.shape[0]\n",
    "            \n",
    "            x = one_hot_encoding(x, char_len)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "    \n",
    "            if(train_on_gpu):\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "                \n",
    "            # Remove the hidden layer from history to avoid back propagating in time\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # Set gradients to 0, avoid gradient accumulation\n",
    "            net.zero_grad()\n",
    "            # store the hidden layer which could be used for the next epoch\n",
    "            output, h = net(x, h)\n",
    "            \n",
    "            loss = F.cross_entropy(output, y.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the gradient to avoid exploding gradients\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optim.step()\n",
    "            \n",
    "            total += batch\n",
    "            sum_loss += batch*(loss.item())\n",
    "            \n",
    "        if j%5 == 0:\n",
    "            print(f\"train loss at epoch {j}: \", sum_loss/total)\n",
    "        # Save the model with parameters that produces lowest validation error\n",
    "        \n",
    "        val_loss = validation(train_on_gpu , net, j, valid_data,batch_size)\n",
    "        if val_loss < acc:\n",
    "            torch.save(net.state_dict(), name)\n",
    "        acc = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:17:00.139367Z",
     "start_time": "2019-05-06T02:17:00.131643Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(train_on_gpu, net, epoch, valid_data , seq_length = 30, batch_size = 256, lr = 0.001, clip = 5):\n",
    "    \"\"\"\n",
    "    Function that returns validation loss.\n",
    "    Called for every epoch.\n",
    "    \"\"\"\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    sum_loss = 0\n",
    "    total = 0\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for i, (x, y) in enumerate(data_loader(valid_data, batch_size, seq_length)):\n",
    "    \n",
    "        batch = y.shape[0]\n",
    "        # One-hot encode our data and make them Torch tensors\n",
    "        x = one_hot_encoding(x, len(net.chars))\n",
    "\n",
    "        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        if(train_on_gpu):\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        \n",
    "        # Remove the hidden state parameter from history\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        output, val_h = net(x, val_h)\n",
    "\n",
    "        val_loss = F.cross_entropy(output, y.view(batch_size*seq_length))        \n",
    "        sum_loss += batch*(val_loss.item())\n",
    "        total += batch\n",
    "    if epoch%5 == 0:\n",
    "        print(\"val loss\", sum_loss/total)\n",
    "    return (sum_loss/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:20:46.384035Z",
     "start_time": "2019-05-06T02:17:01.795008Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss at epoch 0:  3.362160073386298\n",
      "val loss 3.2157922983169556\n",
      "train loss at epoch 5:  3.2859446825804532\n",
      "val loss 3.184307813644409\n",
      "train loss at epoch 10:  2.9332976694460267\n",
      "val loss 2.8943209648132324\n",
      "train loss at epoch 15:  2.486027655778108\n",
      "val loss 2.364353656768799\n",
      "train loss at epoch 20:  2.1597665592476174\n",
      "val loss 2.037652015686035\n",
      "train loss at epoch 25:  1.9363778078997578\n",
      "val loss 1.7812238931655884\n",
      "train loss at epoch 30:  1.7917820745044284\n",
      "val loss 1.653004765510559\n",
      "train loss at epoch 35:  1.6948643922805786\n",
      "val loss 1.5593156218528748\n",
      "train loss at epoch 40:  1.6270457329573456\n",
      "val loss 1.5129769444465637\n",
      "train loss at epoch 45:  1.5805056978155065\n",
      "val loss 1.4686496257781982\n",
      "train loss at epoch 50:  1.5432682081505105\n",
      "val loss 1.4467105865478516\n",
      "train loss at epoch 55:  1.5104516082339816\n",
      "val loss 1.4244187474250793\n",
      "train loss at epoch 60:  1.4857933123906453\n",
      "val loss 1.4144509434700012\n",
      "train loss at epoch 65:  1.4659521138226543\n",
      "val loss 1.3977885246276855\n",
      "train loss at epoch 70:  1.4474133032339591\n",
      "val loss 1.382252037525177\n",
      "train loss at epoch 75:  1.432484953491776\n",
      "val loss 1.379343330860138\n",
      "train loss at epoch 80:  1.420647934631065\n",
      "val loss 1.3721680641174316\n",
      "train loss at epoch 85:  1.4072169109627053\n",
      "val loss 1.3637866973876953\n",
      "train loss at epoch 90:  1.396141153794748\n",
      "val loss 1.3605091571807861\n",
      "train loss at epoch 95:  1.3896040386623807\n",
      "val loss 1.3499590754508972\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(train_on_gpu, net, train_encoded, valid_encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T23:47:09.077030Z",
     "start_time": "2018-12-29T23:47:05.067312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12800, 78])\n",
      "tensor(509.)\n",
      "torch.Size([12800, 78]) torch.Size([12800])\n",
      "tensor(4.3452, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "epochs = 1 # start smaller if you are just testing initial behavior\n",
    "lr=0.001\n",
    "print_every=10\n",
    "data = encoded\n",
    "val_frac = 0.2\n",
    "clip = 5\n",
    "\n",
    "net.train()\n",
    "    \n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create training and validation data\n",
    "val_idx = int(len(data)*(1-val_frac))\n",
    "data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "counter = 0\n",
    "n_chars = len(net.chars)\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    x,y = next(data_loader(data, batch_size, seq_length))\n",
    "        \n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    # One-hot encode our data and make them Torch tensors\n",
    "    x = one_hot_encoding(x, n_chars)\n",
    "\n",
    "\n",
    "    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    net.zero_grad()\n",
    "\n",
    "    # get the output from the model\n",
    "    output, h = net(inputs, h)\n",
    "    print(output.shape)\n",
    "\n",
    "    preds = torch.max(output, dim=1)[1]\n",
    "    preds = preds.reshape(batch_size,-1)\n",
    "    print((preds==targets).float().sum())\n",
    "    print(output.shape, targets.view(batch_size*seq_length).shape)\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "    opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Predictions !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) We pass few characters through the model and do a softmax on the output.  <br>\n",
    "2) This gives us probability distribution of for all the characters in our set. <br>\n",
    "3) We sampe from this distribution randomly and feed it into the model again \n",
    "for predictions until we hit the threshold on generating characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:15:51.770348Z",
     "start_time": "2019-05-06T02:15:51.763670Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(train_on_gpu, net, char, h=None, top_k=None):\n",
    "        \"\"\"\n",
    "        Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "        \"\"\" \n",
    "                \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encoding(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:15:52.191036Z",
     "start_time": "2019-05-06T02:15:52.185830Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(train_on_gpu, net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(train_on_gpu , net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(train_on_gpu, net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T02:21:57.689408Z",
     "start_time": "2019-05-06T02:21:57.008276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry on the stool on the second. Harry saw him stool and shill she seat an owl while though it had been a trink back the sight. Their fame to hungry to thinks they were so how to tell Harry in the starts of his things. \n",
      "\n",
      "\"What wouldn't shat the some with the thing to the from the from you stor it. It's been worrying the frinned that him anything that he's back if they heard to this tonay.\" \"It stopped him to gras in the telephone. \n",
      "\n",
      "\"I were number of steak where the steps with however.\" \n",
      "\n",
      "\"Wouldn't started off the that saying that had tree. It can saying you all?\" \n",
      "\n",
      "The franticing to him. \n",
      "\n",
      "\"It's the game of the still. I heard her store off and there's no one stelled it.\" \n",
      "\n",
      "Harry, and he was nosiced to his both of the fire and flound three of the floor in Harry's stop to be teacher of Harry started. \n",
      "\n",
      "\"Who weren't to get them in his bess in though it.\" \n",
      "\n",
      "\"What's you to him one of his back to the crowds on you? I'm been in the top so to him to get to some something, won't have got to stop hi\n"
     ]
    }
   ],
   "source": [
    "# prime = 'Harry' warms up the network with hidden states\n",
    "print(sample(train_on_gpu, net, 1000, prime='Harry', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
