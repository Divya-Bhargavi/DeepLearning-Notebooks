{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks for MNIST dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:46.431314Z",
          "start_time": "2019-02-18T02:46:45.779868Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:46.961826Z",
          "start_time": "2019-02-18T02:46:46.617835Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading MNIST\n",
        "Here we load the dataset and create data loaders."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = datasets.MNIST('data', train=True, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "test_ds = datasets.MNIST('data', train=False, download=True, \n",
        "                       transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:48.503551Z",
          "start_time": "2019-02-18T02:46:48.410826Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "#batch_size = 5 # for testing\n",
        "kwargs = {'num_workers': 4, 'pin_memory': True} \n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:49.577598Z",
          "start_time": "2019-02-18T02:46:49.571069Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at Examples"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = iter(train_loader)\n",
        "x, y = next(train_dl)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:50.691691Z",
          "start_time": "2019-02-18T02:46:50.546496Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape, y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:51.080344Z",
          "start_time": "2019-02-18T02:46:51.073184Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper method (from fast.ai)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def show(img, title=None):\n",
        "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
        "    if title is not None: plt.title(title)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:56.001759Z",
          "start_time": "2019-02-18T02:46:55.998362Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first from torch to numpy\n",
        "X = x.numpy(); Y = y.numpy()\n",
        "X.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": [
              "(32, 1, 28, 28)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:57.379891Z",
          "start_time": "2019-02-18T02:46:57.371332Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show(X[0][0], Y[0])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADwpJREFUeJzt3X+sFXV6x/HPU4VolPVHFLyytrBITXsJsBVNU5qGRiF0/xD3D9el2NC4lbVAdE0TIZv4K8ZEsYsaJWvYgMs2C2iiCNlqQEnV6h/q1bALLC5LCCtXCFdkA5dGQ5Cnf9yhveI53zmcM3Pm3Pu8X8nNPXeeMzNPJnyYOfPjfM3dBSCeP6m6AQDVIPxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ajKzS81sg5n9j5n9wcz+seqeUKxzq24AHWuFpBOSxkiaKuk/zezX7r6z2rZQFOMOP5zJzC6Q9EdJk9x9dzbtPyR94u5LK20OheGwH7X8uaQvTwc/82tJ3RX1gxIQftRyoaSjZ0w7KmlUBb2gJIQftRyX9I0zpn1DUn8FvaAkhB+17JZ0rplNHDRtiiRO9g0jnPBDTWa2XpJL+hcNnO1/RdLfcLZ/+GDPj3oWSjpfUp+kdZL+leAPL+z5gaDY8wNBEX4gKMIPBEX4gaDa+mCPmXF2ESiZu1sj72tpz29ms83sd2a2x8x44AMYQpq+1Gdm52jgTrCZknolvS9prrv/NjEPe36gZO3Y818vaY+773X3E5LWS5rTwvIAtFEr4R8raf+gv3uzaV9hZgvMrMfMelpYF4CCtXLCr9ahxdcO6919paSVEof9QCdpZc/fK+mqQX9/U9KB1toB0C6thP99SRPNbLyZjZT0fUmbimkLQNmaPux395NmtljSZknnSFrNU1/A0NHWp/r4zA+Ury03+QAYugg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqtQ3QjnkmTJtWtLVmyJDnvvHnzkvVVq1Yl648//njd2u7du5PzRsCeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC4jo/WjJ58uRkfcuWLXVrl19+eXLevBGkb7/99mT91KlTdWtPPPFEct6PPvooWR8OWgq/me2T1C/pS0kn3X1aEU0BKF8Re/6/d/fDBSwHQBvxmR8IqtXwu6QtZvaBmS2o9QYzW2BmPWbW0+K6ABSo1cP+6e5+wMxGS3rNzD5y97cGv8HdV0paKUlmlj6DA6BtWtrzu/uB7HefpA2Sri+iKQDlazr8ZnaBmY06/VrSLEk7imoMQLks71pq3RnNvqWBvb008PFhrbs/kjMPh/1DzPz585P1xx57LFnPu5Zflbzr+N3d3W3qpHjubo28r+nP/O6+V9KUZucHUC0u9QFBEX4gKMIPBEX4gaAIPxBU05f6mloZl/o6Tt4lre3btyfrrfz7ef3115P1adPSD4lefPHFTa/75MmTyfqyZcuS9fvuu6/pdZet0Ut97PmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICiu8w9z9957b7J+9913J+tdXV3J+v79+5P1devW1a099NBDyXnfeOONZD3vPoBW9Pf3J+ut3GNQNq7zA0gi/EBQhB8IivADQRF+ICjCDwRF+IGgGKJ7CDj//POT9enTp9etLV68ODnvFVdc0VRPp61YsSJZz3suHtVhzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQXGdfwi47rrrkvXNmzeXtu5nn302WV++fHlp665SX19f1S2ULnfPb2arzazPzHYMmnapmb1mZr/Pfl9SbpsAitbIYf/PJc0+Y9pSSVvdfaKkrdnfAIaQ3PC7+1uSjpwxeY6kNdnrNZJuLrgvACVr9jP/GHc/KEnuftDMRtd7o5ktkLSgyfUAKEnpJ/zcfaWklRJf4Al0kmYv9R0ysy5Jyn4P/1OjwDDTbPg3SZqfvZ4vaWMx7QBol9zDfjNbJ2mGpMvMrFfSA5IelfSCmf1A0seSbimzyeFu6dL0xZI777yz6WXnjcvw3HPPJesLFy5set158r6nYMSIEaWtO89TTz1V2brbJTf87j63TumGgnsB0Ebc3gsERfiBoAg/EBThB4Ii/EBQPNLbBlOmTEnW8y6njR07tul1r127Nlm/4447ml52q2bPPvN5sa/K226tOHr0aLL+5ptvlrbuTsGeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC4jp/Abq7u5P1V155JVlvdZjsY8eO1a09+eSTLS27TLNmzaps3cePH0/Wd+zYkawPB+z5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAorvM3aNKkSXVrS5YsSc7b1dXV0ro/++yzZH3mzJl1a9u2bWtp3WWaMWNGsm5mLS0/NX+E5/XzsOcHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaC4zt+gjRs31q2NGzcuOW/eMNmHDh1K1m+66aZkvZOv5afkbZe8ep4jR47UrT399NMtLXs4yN3zm9lqM+szsx2Dpj1oZp+Y2bbs5zvltgmgaI0c9v9cUq2hVZ5w96nZT/qragB0nNzwu/tbkuofPwEYklo54bfYzH6TfSy4pN6bzGyBmfWYWU8L6wJQsGbD/1NJEyRNlXRQ0k/qvdHdV7r7NHef1uS6AJSgqfC7+yF3/9LdT0n6maTri20LQNmaCr+ZDX5G9buShv/3HAPDTO51fjNbJ2mGpMvMrFfSA5JmmNlUSS5pn6QflthjRxg5cmRpy164cGGy3tMzdE+XTJgwoW7toosuKnXd99xzT93ae++9V+q6h4Lc8Lv73BqTV5XQC4A24vZeICjCDwRF+IGgCD8QFOEHguKR3sy8efOS9dGjRze97E8//TRZ37NnT9PLrlredlu2bFndWqtDk69fvz5Zf/nll1ta/nDHnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHggpznT/v67Xvv//+ZP3cc5vfVBs2bEjWd+zo3K9DePjhh5P12267LVlv5Vr+2rVrk/VFixYl6/39/U2vOwL2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVJjr/HPn1voS4v939dVXN73sd955J1lfsmRJ08tu1TXXXJOsP/PMM8n6DTfckKznDaN9+PDhurW77rorOe+rr76arB87dixZRxp7fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IqpEhuq+S9AtJV0g6JWmluz9lZpdKel7SOA0M0/09d/9jea12rs8//zxZv/LKK1uq5303fnd3d93a5MmTk/OOHz8+WTezZP3IkSPJ+qpV9Qd0fv7555PzolyN7PlPSvo3d/8LSX8taZGZ/aWkpZK2uvtESVuzvwEMEbnhd/eD7v5h9rpf0i5JYyXNkbQme9saSTeX1SSA4p3VZ34zGyfp25LelTTG3Q9KA/9BSGp+PCsAbdfwvf1mdqGkFyX9yN2P5X0WHDTfAkkLmmsPQFka2vOb2QgNBP+X7v5SNvmQmXVl9S5JfbXmdfeV7j7N3acV0TCAYuSG3wZ28ask7XL35YNKmyTNz17Pl7Sx+PYAlKWRw/7pkv5J0nYz25ZN+7GkRyW9YGY/kPSxpFvKabHz3Xjjjcn6zp0729RJ8Xp7e5P1vEel33777SLbQYFyw+/ub0uq9wE//bA3gI7FHX5AUIQfCIrwA0ERfiAowg8ERfiBoMJ8dfeJEyeS9S+++CJZP++884psp1Cpr88+fvx4ct7Nmzcn67feemtTPaHzsecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAsb4jlQldm1r6VnaVrr702Wd+6dWvd2qhRo1pad+rrrSVp7969yXp/f3/d2ooVK5rqCUOXuzf0HXvs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKK7zA8MM1/kBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFC54Tezq8zsv8xsl5ntNLO7s+kPmtknZrYt+/lO+e0CKEruTT5m1iWpy90/NLNRkj6QdLOk70k67u7/3vDKuMkHKF2jN/nkjtjj7gclHcxe95vZLkljW2sPQNXO6jO/mY2T9G1J72aTFpvZb8xstZldUmeeBWbWY2Y9LXUKoFAN39tvZhdKelPSI+7+kpmNkXRYkkt6WAMfDW7PWQaH/UDJGj3sbyj8ZjZC0q8kbXb35TXq4yT9yt0n5SyH8AMlK+zBHjMzSask7Roc/OxE4GnflbTjbJsEUJ1Gzvb/raT/lrRd0qls8o8lzZU0VQOH/fsk/TA7OZhaFnt+oGSFHvYXhfAD5eN5fgBJhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByv8CzYIcl/WHQ35dl0zpRp/bWqX1J9NasInv7s0bf2Nbn+b+2crMed59WWQMJndpbp/Yl0VuzquqNw34gKMIPBFV1+FdWvP6UTu2tU/uS6K1ZlfRW6Wd+ANWpes8PoCKEHwiqkvCb2Wwz+52Z7TGzpVX0UI+Z7TOz7dmw45WOL5iNgdhnZjsGTbvUzF4zs99nv2uOkVhRbx0xbHtiWPlKt12nDXff9s/8ZnaOpN2SZkrqlfS+pLnu/tu2NlKHme2TNM3dK78hxMz+TtJxSb84PRSamS2TdMTdH83+47zE3Zd0SG8P6iyHbS+pt3rDyv+zKtx2RQ53X4Qq9vzXS9rj7nvd/YSk9ZLmVNBHx3P3tyQdOWPyHElrstdrNPCPp+3q9NYR3P2gu3+Yve6XdHpY+Uq3XaKvSlQR/rGS9g/6u1cVboAaXNIWM/vAzBZU3UwNY04Pi5b9Hl1xP2fKHba9nc4YVr5jtl0zw90XrYrw1xpKqJOuN05397+S9A+SFmWHt2jMTyVN0MAYjgcl/aTKZrJh5V+U9CN3P1ZlL4PV6KuS7VZF+HslXTXo729KOlBBHzW5+4Hsd5+kDRr4mNJJDp0eITn73VdxP//H3Q+5+5fufkrSz1ThtsuGlX9R0i/d/aVscuXbrlZfVW23KsL/vqSJZjbezEZK+r6kTRX08TVmdkF2IkZmdoGkWeq8occ3SZqfvZ4vaWOFvXxFpwzbXm9YeVW87TptuPtK7vDLLmU8KekcSavd/ZG2N1GDmX1LA3t7aeBx57VV9mZm6yTN0MAjn4ckPSDpZUkvSPpTSR9LusXd237irU5vM3SWw7aX1Fu9YeXfVYXbrsjh7gvph9t7gZi4ww8IivADQRF+ICjCDwRF+IGgCD8QFOEHgvpfArpOg5NP1h0AAAAASUVORK5CYII=\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:46:58.560709Z",
          "start_time": "2019-02-18T02:46:58.436775Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0][0][:4][:4])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
            " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
            " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
            " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
            "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]]\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:47:01.651298Z",
          "start_time": "2019-02-18T02:47:01.643849Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Neural Network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for the number of neurons in the hidden unit\n",
        "def get_model(M = 300):\n",
        "    net = nn.Sequential(nn.Linear(28*28, M),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(M, 10))\n",
        "    return net #.cuda()"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:48:39.837017Z",
          "start_time": "2019-02-18T02:48:39.832756Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    total = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):  \n",
        "            batch = images.shape[0] # size of the batch\n",
        "            # Convert torch tensor to Variable, change shape of the input\n",
        "            images = images.view(-1, 28*28).cuda()\n",
        "            labels = labels.cuda()\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()  # zero the gradient buffer\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels.cuda())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "            total += batch\n",
        "            sum_loss += batch * loss.item()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [%d/%d], Loss: %.4f' \n",
        "                   %(epoch+1, num_epochs, sum_loss/total))\n",
        "                \n",
        "        train_loss = sum_loss/total\n",
        "        print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, train_loss))\n",
        "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
        "        print('Epoch [%d/%d], Valid Accuracy: %.4f, Valid Loss: %.4f' %(epoch+1, num_epochs, val_acc, val_loss))\n",
        "    return val_acc, val_loss, train_loss"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:48:41.900824Z",
          "start_time": "2019-02-18T02:48:41.894638Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_accuracy_loss(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    sum_loss = 0.0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.view(-1, 28*28).cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, pred = torch.max(outputs.data, 1)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        sum_loss += labels.size(0)*loss.item()\n",
        "        total += labels.size(0)\n",
        "        correct += pred.eq(labels.data).sum().item()\n",
        "    return 100 * correct / total, sum_loss/ total"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:48:43.693595Z",
          "start_time": "2019-02-18T02:48:43.688562Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_model().cuda()\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "model_accuracy_loss(net, test_loader)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": [
              "(8.5, 2.3198746158599852)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:48:46.277383Z",
          "start_time": "2019-02-18T02:48:44.992618Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, test_loader, num_epochs=2, model=net, optimizer=optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 0.7486\n",
            "Epoch [1/2], Loss: 0.5785\n",
            "Epoch [1/2], Loss: 0.5125\n",
            "Epoch [1/2], Loss: 0.4748\n",
            "Epoch [1/2], Loss: 0.4438\n",
            "Epoch [1/2], Loss: 0.4206\n",
            "Epoch [1/2], Loss: 0.4105\n",
            "Epoch [1/2], Loss: 0.3973\n",
            "Epoch [1/2], Loss: 0.3902\n",
            "Epoch [1/2], Loss: 0.3798\n",
            "Epoch [1/2], Loss: 0.3735\n",
            "Epoch [1/2], Loss: 0.3671\n",
            "Epoch [1/2], Loss: 0.3649\n",
            "Epoch [1/2], Loss: 0.3634\n",
            "Epoch [1/2], Loss: 0.3640\n",
            "Epoch [1/2], Loss: 0.3581\n",
            "Epoch [1/2], Loss: 0.3544\n",
            "Epoch [1/2], Loss: 0.3485\n",
            "Epoch [1/2], Loss: 0.3447\n",
            "Epoch [1/2], Valid Accuracy: 92.6800, Valid Loss: 0.2820\n",
            "Epoch [2/2], Loss: 0.3400\n",
            "Epoch [2/2], Loss: 0.3359\n",
            "Epoch [2/2], Loss: 0.3309\n",
            "Epoch [2/2], Loss: 0.3284\n",
            "Epoch [2/2], Loss: 0.3270\n",
            "Epoch [2/2], Loss: 0.3232\n",
            "Epoch [2/2], Loss: 0.3209\n",
            "Epoch [2/2], Loss: 0.3188\n",
            "Epoch [2/2], Loss: 0.3171\n",
            "Epoch [2/2], Loss: 0.3136\n",
            "Epoch [2/2], Loss: 0.3123\n",
            "Epoch [2/2], Loss: 0.3113\n",
            "Epoch [2/2], Loss: 0.3104\n",
            "Epoch [2/2], Loss: 0.3089\n",
            "Epoch [2/2], Loss: 0.3081\n",
            "Epoch [2/2], Loss: 0.3069\n",
            "Epoch [2/2], Loss: 0.3053\n",
            "Epoch [2/2], Loss: 0.3032\n",
            "Epoch [2/2], Loss: 0.3026\n",
            "Epoch [2/2], Valid Accuracy: 93.0300, Valid Loss: 0.2764\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": [
              "(93.03, 0.276409143280983, 0.302553220307827)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:49:31.685163Z",
          "start_time": "2019-02-18T02:48:53.179774Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models with L2 regularization\n",
        "To add L2 regularization use the `weight_decay` argument on the optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay = 0.01)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:49:34.223464Z",
          "start_time": "2019-02-18T02:49:34.220059Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models with Dropout"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_v2(M = 300, p=0):\n",
        "    modules = []\n",
        "    modules.append(nn.Linear(28*28, M))\n",
        "    modules.append(nn.ReLU())\n",
        "    if p > 0:\n",
        "        modules.append(nn.Dropout(p))\n",
        "    modules.append(nn.Linear(M, 10))\n",
        "    return nn.Sequential(*modules) #.cuda()"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:49:55.290034Z",
          "start_time": "2019-02-18T02:49:55.286046Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_model_v2(M = 300, p=0.1).cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:49:57.562415Z",
          "start_time": "2019-02-18T02:49:57.555569Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, test_loader, num_epochs=4, model=net, optimizer=optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 0.7191\n",
            "Epoch [1/4], Loss: 0.5809\n",
            "Epoch [1/4], Loss: 0.5241\n",
            "Epoch [1/4], Loss: 0.5067\n",
            "Epoch [1/4], Loss: 0.4952\n",
            "Epoch [1/4], Loss: 0.4795\n",
            "Epoch [1/4], Loss: 0.4704\n",
            "Epoch [1/4], Loss: 0.4627\n",
            "Epoch [1/4], Loss: 0.4533\n",
            "Epoch [1/4], Loss: 0.4485\n",
            "Epoch [1/4], Loss: 0.4426\n",
            "Epoch [1/4], Loss: 0.4351\n",
            "Epoch [1/4], Loss: 0.4332\n",
            "Epoch [1/4], Loss: 0.4304\n",
            "Epoch [1/4], Loss: 0.4258\n",
            "Epoch [1/4], Loss: 0.4229\n",
            "Epoch [1/4], Loss: 0.4220\n",
            "Epoch [1/4], Loss: 0.4206\n",
            "Epoch [1/4], Loss: 0.4192\n",
            "Epoch [1/4], Valid Accuracy: 91.9100, Valid Loss: 0.3146\n",
            "Epoch [2/4], Loss: 0.4123\n",
            "Epoch [2/4], Loss: 0.4052\n",
            "Epoch [2/4], Loss: 0.3987\n",
            "Epoch [2/4], Loss: 0.3942\n",
            "Epoch [2/4], Loss: 0.3895\n",
            "Epoch [2/4], Loss: 0.3830\n",
            "Epoch [2/4], Loss: 0.3775\n",
            "Epoch [2/4], Loss: 0.3761\n",
            "Epoch [2/4], Loss: 0.3730\n",
            "Epoch [2/4], Loss: 0.3695\n",
            "Epoch [2/4], Loss: 0.3666\n",
            "Epoch [2/4], Loss: 0.3635\n",
            "Epoch [2/4], Loss: 0.3636\n",
            "Epoch [2/4], Loss: 0.3612\n",
            "Epoch [2/4], Loss: 0.3597\n",
            "Epoch [2/4], Loss: 0.3583\n",
            "Epoch [2/4], Loss: 0.3561\n",
            "Epoch [2/4], Loss: 0.3534\n",
            "Epoch [2/4], Loss: 0.3536\n",
            "Epoch [2/4], Valid Accuracy: 90.3900, Valid Loss: 0.4066\n",
            "Epoch [3/4], Loss: 0.3516\n",
            "Epoch [3/4], Loss: 0.3493\n",
            "Epoch [3/4], Loss: 0.3474\n",
            "Epoch [3/4], Loss: 0.3455\n",
            "Epoch [3/4], Loss: 0.3431\n",
            "Epoch [3/4], Loss: 0.3408\n",
            "Epoch [3/4], Loss: 0.3376\n",
            "Epoch [3/4], Loss: 0.3372\n",
            "Epoch [3/4], Loss: 0.3343\n",
            "Epoch [3/4], Loss: 0.3333\n",
            "Epoch [3/4], Loss: 0.3312\n",
            "Epoch [3/4], Loss: 0.3297\n",
            "Epoch [3/4], Loss: 0.3290\n",
            "Epoch [3/4], Loss: 0.3271\n",
            "Epoch [3/4], Loss: 0.3260\n",
            "Epoch [3/4], Loss: 0.3247\n",
            "Epoch [3/4], Loss: 0.3246\n",
            "Epoch [3/4], Loss: 0.3242\n",
            "Epoch [3/4], Loss: 0.3242\n",
            "Epoch [3/4], Valid Accuracy: 94.1500, Valid Loss: 0.2772\n",
            "Epoch [4/4], Loss: 0.3223\n",
            "Epoch [4/4], Loss: 0.3207\n",
            "Epoch [4/4], Loss: 0.3189\n",
            "Epoch [4/4], Loss: 0.3175\n",
            "Epoch [4/4], Loss: 0.3167\n",
            "Epoch [4/4], Loss: 0.3156\n",
            "Epoch [4/4], Loss: 0.3146\n",
            "Epoch [4/4], Loss: 0.3136\n",
            "Epoch [4/4], Loss: 0.3126\n",
            "Epoch [4/4], Loss: 0.3115\n",
            "Epoch [4/4], Loss: 0.3108\n",
            "Epoch [4/4], Loss: 0.3102\n",
            "Epoch [4/4], Loss: 0.3086\n",
            "Epoch [4/4], Loss: 0.3083\n",
            "Epoch [4/4], Loss: 0.3074\n",
            "Epoch [4/4], Loss: 0.3068\n",
            "Epoch [4/4], Loss: 0.3063\n",
            "Epoch [4/4], Loss: 0.3052\n",
            "Epoch [4/4], Loss: 0.3044\n",
            "Epoch [4/4], Valid Accuracy: 93.3400, Valid Loss: 0.3389\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": [
              "(93.34, 0.3389083451986313, 0.30440951741337774)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:51:26.882186Z",
          "start_time": "2019-02-18T02:50:06.894200Z"
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different learning rates"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
        "    model.train()\n",
        "    sum_loss = 0.0\n",
        "    total = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):  \n",
        "            batch = images.shape[0] # size of the batch\n",
        "            # Convert torch tensor to Variable, change shape of the input\n",
        "            images = images.view(-1, 28*28).cuda()\n",
        "            labels = labels.cuda()\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()  # zero the gradient buffer\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels.cuda())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "            total += batch\n",
        "            sum_loss += batch * loss.item()\n",
        "#             if (i+1) % 100 == 0:\n",
        "#                 print ('Epoch [%d/%d], Loss: %.4f' \n",
        "#                    %(epoch+1, num_epochs, sum_loss/total))\n",
        "                \n",
        "        train_loss = sum_loss/total\n",
        "#         print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, train_loss))\n",
        "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
        "#         print('Epoch [%d/%d], Valid Accuracy: %.4f, Valid Loss: %.4f' %(epoch+1, num_epochs, val_acc, val_loss))\n",
        "    return val_acc, val_loss, train_loss"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T02:59:29.453364Z",
          "start_time": "2019-02-18T02:59:29.447907Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "lrs = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "val_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "lr_list = []\n",
        "\n",
        "for l in lrs:\n",
        "    start = time.time()\n",
        "    net = get_model_v2(M = 300, p=0.1).cuda()\n",
        "    optimizer = optim.Adam(net.parameters(), lr= l)\n",
        "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs = 10, model=net, optimizer=optimizer)\n",
        "    end = time.time()\n",
        "    lr_list.append(l)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print('lr of value : ',l, ' finished in ', end-start, ' secs')\n",
        "    \n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr of value :  1  finished in  96.86830401420593  secs\n",
            "lr of value :  0.1  finished in  97.49245381355286  secs\n",
            "lr of value :  0.01  finished in  95.55545020103455  secs\n",
            "lr of value :  0.001  finished in  94.7211492061615  secs\n",
            "lr of value :  0.0001  finished in  94.26864171028137  secs\n",
            "lr of value :  1e-05  finished in  97.41456198692322  secs\n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-18T03:03:19.336026Z",
          "start_time": "2019-02-18T02:59:39.996301Z"
        },
        "run_control": {
          "marked": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lr_list)\n",
        "print(train_loss_list)\n",
        "print(val_loss_list)\n",
        "print(val_acc_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0.1, 0.01, 0.001, 0.0001, 1e-05]\n",
            "[6.715851014779409, 2.3658197514724733, 0.2538151095507542, 0.057895181021727624, 0.12552360342601934, 0.3919103501987457]\n",
            "[2.72065295791626, 2.311489500427246, 0.32766912140846255, 0.12037180772423745, 0.06759599306583404, 0.2236744359254837]\n",
            "[11.37, 11.38, 94.36, 97.72, 97.9, 93.78]\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* The highest validation accuracy occurs at lr = 1e-4 followed by 1e-3. Interpolated value would be 5*1e-4\n",
        "* However 1e-4 has underfitting since train loss is more than valid loss.\n",
        "* We could train it for more epochs to get much higher accuracy. I would choose 1e-4\n",
        "* 1e-3 has overfitting since after 10 epochs there is significant gap between train and valid loss."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_model_v2(M = 300, p=0.1).cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr= 5e-4)\n",
        "train_model(train_loader, test_loader, num_epochs=4, model=net, optimizer=optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 0.7857\n",
            "Epoch [1/4], Loss: 0.5903\n",
            "Epoch [1/4], Loss: 0.4985\n",
            "Epoch [1/4], Loss: 0.4481\n",
            "Epoch [1/4], Loss: 0.4097\n",
            "Epoch [1/4], Loss: 0.3824\n",
            "Epoch [1/4], Loss: 0.3629\n",
            "Epoch [1/4], Loss: 0.3420\n",
            "Epoch [1/4], Loss: 0.3274\n",
            "Epoch [1/4], Loss: 0.3168\n",
            "Epoch [1/4], Loss: 0.3036\n",
            "Epoch [1/4], Loss: 0.2929\n",
            "Epoch [1/4], Loss: 0.2829\n",
            "Epoch [1/4], Loss: 0.2746\n",
            "Epoch [1/4], Loss: 0.2661\n",
            "Epoch [1/4], Loss: 0.2599\n",
            "Epoch [1/4], Loss: 0.2536\n",
            "Epoch [1/4], Loss: 0.2467\n",
            "Epoch [1/4], Loss: 0.2428\n",
            "Epoch [1/4], Valid Accuracy: 96.4900, Valid Loss: 0.1173\n",
            "Epoch [2/4], Loss: 0.2368\n",
            "Epoch [2/4], Loss: 0.2295\n",
            "Epoch [2/4], Loss: 0.2243\n",
            "Epoch [2/4], Loss: 0.2190\n",
            "Epoch [2/4], Loss: 0.2139\n",
            "Epoch [2/4], Loss: 0.2093\n",
            "Epoch [2/4], Loss: 0.2050\n",
            "Epoch [2/4], Loss: 0.2007\n",
            "Epoch [2/4], Loss: 0.1974\n",
            "Epoch [2/4], Loss: 0.1940\n",
            "Epoch [2/4], Loss: 0.1905\n",
            "Epoch [2/4], Loss: 0.1873\n",
            "Epoch [2/4], Loss: 0.1842\n",
            "Epoch [2/4], Loss: 0.1817\n",
            "Epoch [2/4], Loss: 0.1790\n",
            "Epoch [2/4], Loss: 0.1762\n",
            "Epoch [2/4], Loss: 0.1739\n",
            "Epoch [2/4], Loss: 0.1715\n",
            "Epoch [2/4], Loss: 0.1697\n",
            "Epoch [2/4], Valid Accuracy: 97.3200, Valid Loss: 0.0854\n",
            "Epoch [3/4], Loss: 0.1669\n",
            "Epoch [3/4], Loss: 0.1646\n",
            "Epoch [3/4], Loss: 0.1619\n",
            "Epoch [3/4], Loss: 0.1594\n",
            "Epoch [3/4], Loss: 0.1569\n",
            "Epoch [3/4], Loss: 0.1548\n",
            "Epoch [3/4], Loss: 0.1530\n",
            "Epoch [3/4], Loss: 0.1512\n",
            "Epoch [3/4], Loss: 0.1495\n",
            "Epoch [3/4], Loss: 0.1475\n",
            "Epoch [3/4], Loss: 0.1461\n",
            "Epoch [3/4], Loss: 0.1446\n",
            "Epoch [3/4], Loss: 0.1429\n",
            "Epoch [3/4], Loss: 0.1415\n",
            "Epoch [3/4], Loss: 0.1400\n",
            "Epoch [3/4], Loss: 0.1385\n",
            "Epoch [3/4], Loss: 0.1373\n",
            "Epoch [3/4], Loss: 0.1359\n",
            "Epoch [3/4], Loss: 0.1350\n",
            "Epoch [3/4], Valid Accuracy: 97.6300, Valid Loss: 0.0758\n",
            "Epoch [4/4], Loss: 0.1333\n",
            "Epoch [4/4], Loss: 0.1319\n",
            "Epoch [4/4], Loss: 0.1304\n",
            "Epoch [4/4], Loss: 0.1290\n",
            "Epoch [4/4], Loss: 0.1276\n",
            "Epoch [4/4], Loss: 0.1261\n",
            "Epoch [4/4], Loss: 0.1249\n",
            "Epoch [4/4], Loss: 0.1236\n",
            "Epoch [4/4], Loss: 0.1223\n",
            "Epoch [4/4], Loss: 0.1213\n",
            "Epoch [4/4], Loss: 0.1203\n",
            "Epoch [4/4], Loss: 0.1192\n",
            "Epoch [4/4], Loss: 0.1182\n",
            "Epoch [4/4], Loss: 0.1171\n",
            "Epoch [4/4], Loss: 0.1163\n",
            "Epoch [4/4], Loss: 0.1155\n",
            "Epoch [4/4], Loss: 0.1145\n",
            "Epoch [4/4], Loss: 0.1138\n",
            "Epoch [4/4], Loss: 0.1130\n",
            "Epoch [4/4], Valid Accuracy: 97.8000, Valid Loss: 0.0718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": [
              "(97.8, 0.07184602416753769, 0.11301818922708431)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_model_v2(M = 300, p=0.1).cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr= 5e-2)\n",
        "train_model(train_loader, test_loader, num_epochs=4, model=net, optimizer=optimizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 3.6033\n",
            "Epoch [1/4], Loss: 2.5936\n",
            "Epoch [1/4], Loss: 2.2497\n",
            "Epoch [1/4], Loss: 2.0579\n",
            "Epoch [1/4], Loss: 1.9637\n",
            "Epoch [1/4], Loss: 1.8969\n",
            "Epoch [1/4], Loss: 1.8437\n",
            "Epoch [1/4], Loss: 1.8183\n",
            "Epoch [1/4], Loss: 1.7832\n",
            "Epoch [1/4], Loss: 1.7738\n",
            "Epoch [1/4], Loss: 1.7541\n",
            "Epoch [1/4], Loss: 1.7400\n",
            "Epoch [1/4], Loss: 1.7268\n",
            "Epoch [1/4], Loss: 1.7209\n",
            "Epoch [1/4], Loss: 1.7166\n",
            "Epoch [1/4], Loss: 1.7211\n",
            "Epoch [1/4], Loss: 1.7372\n",
            "Epoch [1/4], Loss: 1.7486\n",
            "Epoch [1/4], Loss: 1.7525\n",
            "Epoch [1/4], Valid Accuracy: 33.4400, Valid Loss: 1.7334\n",
            "Epoch [2/4], Loss: 1.7547\n",
            "Epoch [2/4], Loss: 1.7631\n",
            "Epoch [2/4], Loss: 1.7639\n",
            "Epoch [2/4], Loss: 1.7649\n",
            "Epoch [2/4], Loss: 1.7603\n",
            "Epoch [2/4], Loss: 1.7586\n",
            "Epoch [2/4], Loss: 1.7563\n",
            "Epoch [2/4], Loss: 1.7591\n",
            "Epoch [2/4], Loss: 1.7584\n",
            "Epoch [2/4], Loss: 1.7610\n",
            "Epoch [2/4], Loss: 1.7595\n",
            "Epoch [2/4], Loss: 1.7588\n",
            "Epoch [2/4], Loss: 1.7575\n",
            "Epoch [2/4], Loss: 1.7602\n",
            "Epoch [2/4], Loss: 1.7563\n",
            "Epoch [2/4], Loss: 1.7539\n",
            "Epoch [2/4], Loss: 1.7599\n",
            "Epoch [2/4], Loss: 1.7569\n",
            "Epoch [2/4], Loss: 1.7562\n",
            "Epoch [2/4], Valid Accuracy: 39.7000, Valid Loss: 1.6115\n",
            "Epoch [3/4], Loss: 1.7525\n",
            "Epoch [3/4], Loss: 1.7514\n",
            "Epoch [3/4], Loss: 1.7488\n",
            "Epoch [3/4], Loss: 1.7460\n",
            "Epoch [3/4], Loss: 1.7410\n",
            "Epoch [3/4], Loss: 1.7401\n",
            "Epoch [3/4], Loss: 1.7361\n",
            "Epoch [3/4], Loss: 1.7346\n",
            "Epoch [3/4], Loss: 1.7305\n",
            "Epoch [3/4], Loss: 1.7276\n",
            "Epoch [3/4], Loss: 1.7261\n",
            "Epoch [3/4], Loss: 1.7263\n",
            "Epoch [3/4], Loss: 1.7240\n",
            "Epoch [3/4], Loss: 1.7220\n",
            "Epoch [3/4], Loss: 1.7204\n",
            "Epoch [3/4], Loss: 1.7209\n",
            "Epoch [3/4], Loss: 1.7181\n",
            "Epoch [3/4], Loss: 1.7165\n",
            "Epoch [3/4], Loss: 1.7151\n",
            "Epoch [3/4], Valid Accuracy: 40.7100, Valid Loss: 1.6227\n",
            "Epoch [4/4], Loss: 1.7148\n",
            "Epoch [4/4], Loss: 1.7158\n",
            "Epoch [4/4], Loss: 1.7143\n",
            "Epoch [4/4], Loss: 1.7125\n",
            "Epoch [4/4], Loss: 1.7121\n",
            "Epoch [4/4], Loss: 1.7101\n",
            "Epoch [4/4], Loss: 1.7075\n",
            "Epoch [4/4], Loss: 1.7057\n",
            "Epoch [4/4], Loss: 1.7033\n",
            "Epoch [4/4], Loss: 1.7027\n",
            "Epoch [4/4], Loss: 1.7045\n",
            "Epoch [4/4], Loss: 1.7060\n",
            "Epoch [4/4], Loss: 1.7076\n",
            "Epoch [4/4], Loss: 1.7078\n",
            "Epoch [4/4], Loss: 1.7085\n",
            "Epoch [4/4], Loss: 1.7086\n",
            "Epoch [4/4], Loss: 1.7095\n",
            "Epoch [4/4], Loss: 1.7092\n",
            "Epoch [4/4], Loss: 1.7098\n",
            "Epoch [4/4], Valid Accuracy: 34.4900, Valid Loss: 1.7246\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": [
              "(34.49, 1.7245856410980225, 1.709817103354136)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different M"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_v2(M = 300, p=0):\n",
        "    modules = []\n",
        "    modules.append(nn.Linear(28*28, M))\n",
        "    modules.append(nn.ReLU())\n",
        "    if p > 0:\n",
        "        modules.append(nn.Dropout(p))\n",
        "    modules.append(nn.Linear(M, 10))\n",
        "    return nn.Sequential(*modules) #.cuda()"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "m_list = []\n",
        "\n",
        "for m in [10, 50, 100, 300, 1000, 2000]:\n",
        "    start = time.time()\n",
        "    net = get_model_v2(M = m, p=0.1).cuda()\n",
        "    optimizer = optim.Adam(net.parameters(), lr= 0.01)\n",
        "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
        "    end = time.time()\n",
        "    m_list.append(m)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print('m of value : ',m, ' finished in ', end-start, ' secs')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m of value :  10  finished in  93.06841278076172  secs\n",
            "m of value :  50  finished in  91.42460584640503  secs\n",
            "m of value :  100  finished in  93.33778929710388  secs\n",
            "m of value :  300  finished in  96.1435878276825  secs\n",
            "m of value :  1000  finished in  94.00119709968567  secs\n",
            "m of value :  2000  finished in  94.15414667129517  secs\n"
          ]
        }
      ],
      "execution_count": 56,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(m_list)\n",
        "print(train_loss_list)\n",
        "print(val_loss_list)\n",
        "print(val_acc_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 50, 100, 300, 1000, 2000]\n",
            "[0.5268350938767195, 0.25601559507350125, 0.25069431071341036, 0.2527236468789975, 0.2619733847383658, 0.26501839753011863]\n",
            "[0.4530211345911026, 0.325758491563797, 0.3040672090291977, 0.31717646474838257, 0.35150576248168947, 0.3478338025569916]\n",
            "[86.9, 92.83, 94.49, 94.01, 93.84, 93.95]\n"
          ]
        }
      ],
      "execution_count": 57,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* High value of m doesnt necessarily give high accuracy. M=100 seems more than enough to get a good accuracy\n",
        "* As the value of M increases we see that the gap between train and test loss increases, this is a sign of overfitting. Infact all the M values have overfitting to some extent, M=100 seems to have low loss and difference in them"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different weight decay"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "w_list = []\n",
        "\n",
        "for w in [0, 0.0001, 0.001, 0.01, 0.1, 0.3]:\n",
        "    start = time.time()\n",
        "    net = get_model_v2(M = 300, p=0.1).cuda()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay = w)\n",
        "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs= 20, model=net, optimizer=optimizer)\n",
        "    end = time.time()\n",
        "    w_list.append(w)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print('w of value : ',w, ' finished in ', end-start, ' secs')\n",
        "    \n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w of value :  0  finished in  190.87114310264587  secs\n",
            "w of value :  0.0001  finished in  190.3960416316986  secs\n",
            "w of value :  0.001  finished in  191.3861222267151  secs\n",
            "w of value :  0.01  finished in  190.59041953086853  secs\n",
            "w of value :  0.1  finished in  190.22906470298767  secs\n",
            "w of value :  0.3  finished in  189.7286970615387  secs\n"
          ]
        }
      ],
      "execution_count": 62,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights, train loss, validation loss and corresponding accuracy in order"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(w_list)\n",
        "print(train_loss_list)\n",
        "print(val_loss_list)\n",
        "print(val_acc_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0.0001, 0.001, 0.01, 0.1, 0.3]\n",
            "[0.03736692767158694, 0.04608046758000429, 0.08024183318475882, 0.17785113293965657, 0.47795682656089467, 0.8252347141687075]\n",
            "[0.12801459040492774, 0.0981404365748167, 0.08314272434711456, 0.15468035526275634, 0.43538561849594115, 0.7967260702133179]\n",
            "[98.01, 97.63, 97.4, 95.96, 89.73, 83.82]\n"
          ]
        }
      ],
      "execution_count": 63,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* As we increase the weight decay we see that the difference between train and test lost and narrowing.\n",
        "* Increasing weight decay regularizes the model and prevents overfitting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different drop outs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "p_list = []\n",
        "\n",
        "for p in range(1,11,2):\n",
        "    start = time.time()\n",
        "    net = get_model_v2(M = 300, p= p/10).cuda()\n",
        "    optimizer = optim.Adam(net.parameters(), lr= 0.001)\n",
        "    val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs=20, model=net, optimizer=optimizer)\n",
        "    end = time.time()\n",
        "    p_list.append(p/10)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print('p of value : ',p*.1, ' finished in ', end-start, ' secs')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p of value :  0.1  finished in  199.75901675224304  secs\n",
            "p of value :  0.30000000000000004  finished in  202.6267592906952  secs\n",
            "p of value :  0.5  finished in  200.17301273345947  secs\n",
            "p of value :  0.7000000000000001  finished in  200.4196605682373  secs\n",
            "p of value :  0.9  finished in  203.41671013832092  secs\n"
          ]
        }
      ],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_acc_list)\n",
        "print(val_loss_list)\n",
        "print(train_loss_list)\n",
        "print(p_list)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98.06, 97.9, 98.0, 97.77, 98.12]\n",
            "[0.12519292316585778, 0.1423546568222344, 0.1484976174585521, 0.15545272355973722, 0.13591364087313412]\n",
            "[0.037634527406005654, 0.03948624798679802, 0.041978019182002946, 0.05022911171152373, 0.07868764867369978]\n",
            "[0.1, 0.3, 0.5, 0.7, 0.9]\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* All the pvalues seem to have underfitting for the given value of learning rate.\n",
        "* A pvalue of 0.9 which gives highest accuracy, seems way to high for regularizarion, may be fewer neurons are required to predict this simple data set"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Dropouts give consistently higher accuracy on test data than L2 regularization"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* What we learnt from the past experiments is that dropout work better than L2 norm in regularization .\n",
        "* Learning rate of 1e-3 or lesser is suitable for this data set\n",
        "* Increasing the complexity by adding more neurons doesnt help. Thus for the second layer we will use, less than 100 neurons"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**3 layer neural network**\n",
        "* Explore with different values of M for model complexity and drop out (p) for regularization at lr = 1e-3 for first 10 epochs and lr = 1e-4 for last 10 epochs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_v3(M , p=0):\n",
        "    modules = []\n",
        "    modules.append(nn.Linear(28*28, M[0]))\n",
        "    modules.append(nn.ReLU())\n",
        "    if p > 0:\n",
        "        modules.append(nn.Dropout(p))\n",
        "    modules.append(nn.Linear(M[0], M[1]))\n",
        "    modules.append(nn.ReLU())\n",
        "    if p > 0:\n",
        "        modules.append(nn.Dropout(p))\n",
        "    modules.append(nn.Linear(M[1], 10))\n",
        "    return nn.Sequential(*modules) #.cuda()"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "v_a = []\n",
        "v_l = []\n",
        "t_l = []\n",
        "M_l = []\n",
        "p_l = []\n",
        "for M in [(300,50),(200,50),(100,50),(300,30),(200,30),(100,30) ]:\n",
        "    for p in [0,0.3,0.6,0.9]:\n",
        "        print('M:',M,' p: ',p)\n",
        "        net = get_model_v3(M = M , p= p ).cuda()\n",
        "        optimizer = optim.Adam(net.parameters(), lr= 0.001)\n",
        "        val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs= 10, model=net, optimizer=optimizer)\n",
        "        optimizer = optim.Adam(net.parameters(), lr= 0.001/10)\n",
        "        val_acc, val_loss, train_loss = train_model(train_loader, test_loader, num_epochs= 10, model=net, optimizer=optimizer)\n",
        "        print('Val loss: ', val_loss, ' train loss: ', train_loss, 'Val acc: ', val_acc)\n",
        "        v_a.append(val_acc)\n",
        "        v_l.append(val_loss)\n",
        "        t_l.append(train_loss)\n",
        "        M_l.append(M)\n",
        "        p_l.append(p)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M: (300, 50)  p:  0\n",
            "Val loss:  0.10497654397487641  train loss:  0.0011925701274474463 Val acc:  98.51\n",
            "M: (300, 50)  p:  0.3\n",
            "Val loss:  0.08720177454948426  train loss:  0.013398607277969519 Val acc:  98.65\n",
            "M: (300, 50)  p:  0.6\n",
            "Val loss:  0.09738736186623573  train loss:  0.0991202541384101 Val acc:  98.38\n",
            "M: (300, 50)  p:  0.9\n",
            "Val loss:  0.08702810008525848  train loss:  1.052563990974898 Val acc:  97.96\n",
            "M: (200, 50)  p:  0\n",
            "Val loss:  0.10517750310897828  train loss:  0.0013800404571493467 Val acc:  98.38\n",
            "M: (200, 50)  p:  0.3\n",
            "Val loss:  0.09688276602327824  train loss:  0.01718268789857626 Val acc:  98.52\n",
            "M: (200, 50)  p:  0.6\n",
            "Val loss:  0.08872469582855702  train loss:  0.1340936478992303 Val acc:  98.39\n",
            "M: (200, 50)  p:  0.9\n",
            "Val loss:  0.08800620893239974  train loss:  1.3761993934600303 Val acc:  97.62\n",
            "M: (100, 50)  p:  0\n",
            "Val loss:  0.0917570155620575  train loss:  0.002572277473707994 Val acc:  98.14\n",
            "M: (100, 50)  p:  0.3\n",
            "Val loss:  0.08755594239234925  train loss:  0.03460775689718624 Val acc:  98.19\n",
            "M: (100, 50)  p:  0.6\n",
            "Val loss:  0.09238703132867813  train loss:  0.17705626971344154 Val acc:  98.04\n",
            "M: (100, 50)  p:  0.9\n",
            "Val loss:  0.10898862535357476  train loss:  1.445260082716768 Val acc:  96.74\n",
            "M: (300, 30)  p:  0\n",
            "Val loss:  0.09329524426460266  train loss:  0.0014253947035471597 Val acc:  98.47\n",
            "M: (300, 30)  p:  0.3\n",
            "Val loss:  0.08798905473947526  train loss:  0.017981630193591117 Val acc:  98.48\n",
            "M: (300, 30)  p:  0.6\n",
            "Val loss:  0.08871279201507569  train loss:  0.1260602472668886 Val acc:  98.47\n",
            "M: (300, 30)  p:  0.9\n",
            "Val loss:  0.08265928475260735  train loss:  1.349987357964168 Val acc:  98.17\n",
            "M: (200, 30)  p:  0\n",
            "Val loss:  0.09160295382738114  train loss:  0.0016355303329229356 Val acc:  98.34\n",
            "M: (200, 30)  p:  0.3\n",
            "Val loss:  0.08769417327642441  train loss:  0.023104876729647318 Val acc:  98.53\n",
            "M: (200, 30)  p:  0.6\n",
            "Val loss:  0.08774972125291824  train loss:  0.14714528253982465 Val acc:  98.33\n",
            "M: (200, 30)  p:  0.9\n",
            "Val loss:  0.0872012857735157  train loss:  1.3032313084879084 Val acc:  97.95\n",
            "M: (100, 30)  p:  0\n",
            "Val loss:  0.08531575529575348  train loss:  0.0030679370846350986 Val acc:  98.07\n",
            "M: (100, 30)  p:  0.3\n",
            "Val loss:  0.09765745461583138  train loss:  0.04561501155341665 Val acc:  98.16\n",
            "M: (100, 30)  p:  0.6\n",
            "Val loss:  0.09410814090967179  train loss:  0.20894914136687914 Val acc:  98.03\n",
            "M: (100, 30)  p:  0.9\n",
            "Val loss:  0.10178218767642974  train loss:  1.8173036539578935 Val acc:  97.14\n"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest accuracy seems to come from M= (300,50) for first and second layer with drop out of 0.3"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": true,
      "base_numbering": 1,
      "toc_section_display": "block",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}